{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1bbb4c276d2f39234c50e041ccf7173869f23ad4"
   },
   "source": [
    "# Using LSTM Network With Attention To Recognize Emotions\nIn this notebook, we will build an emotion classifier based on LSTMs and Attention mechanism using Keras library and a publicly available [dataset](https://github.com/huseinzol05/NLP-Dataset/tree/master/emotion-english).\n\nRecurrent neural networks, LSTM and GRU in particular,  are widely used in many natural language processing applications such as classification and language modeling. Attention mechanism is also very popular in these days especially in machine translation where the words in source and target sentences need to be aligned.  \nIn classification tasks (such as emotion recognition), the input words are processed by LSTM networks sequentially and the last output of the LSTM represents the meaning of sentence. However,  in an attention mechanism, weighted average is taken over the outputs in each time step. The model learns to how to generate the weigths according to input sequence. In this way, the model learns where to attend in input sentence. This is very useful when you are tranlating a sentence. The translation model attends a position in source language while generating each words in target language.\n\n\n![Attention Mechanism](https://image.ibb.co/iJ7WRL/attention.jpg)\n**Attention Mechanism**\n\n*Picture is taken from [Feed-forward networks with attention can solve some long-term memory problems](https://arxiv.org/pdf/1512.08756)*\n\n\nIn this study, we will see attention mechanism can be useful for classification tasks as well. The prediction of the model can be interpretable with attention because we can highlight the attended words to understand why the model makes these predictions. As we will see in the last chapter, the outputs will look more fascinating.\n\n![](https://preview.ibb.co/exVpgL/Capture.png)\n\nby [Eray Yildiz](https://twitter.com/erayildiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "887cc6be0765929d5e382a830efdb902bd9ce99b"
   },
   "source": [
    "## Emotion Dataset\nIn this notebook, we are working on an emotion classification dataset which contains tweets labeled into 6 categories (joy, sadness, anger, fear, love, surprise).\n\n### Let's start exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "#Loading the dataset\n",
    "dataset = pd.read_csv(\"../input/emotion.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "96eae56ebe6b927f996e4ca45c67f584b230fc61"
   },
   "outputs": [],
   "source": [
    "# Plot label histogram\n",
    "dataset.emotions.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prin some samples\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b2e3ea8267e3bac72a30ce7803413c684bc8b9a4"
   },
   "source": [
    "## Preparing data for model training\n### Tokenization\nSince the data is already tokenized and lowercased, we just need to split the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "cb739f05cfb4b5d74702cdef1ea5a130c0d90132"
   },
   "outputs": [],
   "source": [
    "input_sentences = [text.split(\" \") for text in dataset[\"text\"].values.tolist()]\n",
    "labels = dataset[\"emotions\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a7c2e03d7e839b2872785157153e0bfef82b0bd"
   },
   "source": [
    "### Creating Vocabulary (word index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "f60be75ae0d5cbfc36eeba0243407b66741bb42e"
   },
   "outputs": [],
   "source": [
    "# Initialize word2id and label2id dictionaries that will be used to encode words and labels\n",
    "word2id = dict()\n",
    "label2id = dict()\n",
    "\n",
    "max_words = 0 # maximum number of words in a sentence\n",
    "\n",
    "# Construction of word2id dict\n",
    "for sentence in input_sentences:\n",
    "    for word in sentence:\n",
    "        # Add words to word2id dict if not exist\n",
    "        if word not in word2id:\n",
    "            word2id[word] = len(word2id)\n",
    "    # If length of the sentence is greater than max_words, update max_words\n",
    "    if len(sentence) > max_words:\n",
    "        max_words = len(sentence)\n",
    "    \n",
    "# Construction of label2id and id2label dicts\n",
    "label2id = {l: i for i, l in enumerate(set(labels))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d984e58ffd25530ac4c05ce623d9237a35cf903d"
   },
   "source": [
    "### Encoding samples with corresponing integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "378ef884a6ebb19b02a70082bc6c854c51780af3"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# Encode input words and labels\n",
    "X = [[word2id[word] for word in sentence] for sentence in input_sentences]\n",
    "Y = [label2id[label] for label in labels]\n",
    "\n",
    "# Apply Padding to X\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, max_words)\n",
    "\n",
    "# Convert Y to numpy array\n",
    "Y = keras.utils.to_categorical(Y, num_classes=len(label2id), dtype='float32')\n",
    "\n",
    "# Print shapes\n",
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of Y: {}\".format(Y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4bccaa5b813414ad7929522d4d0f74dbb9c4c5af"
   },
   "source": [
    "## Build LSTM model with attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "4c1b5fc7613a0fe5a8067135e2de07e0765f1b78"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100 # The dimension of word embeddings\n",
    "\n",
    "# Define input tensor\n",
    "sequence_input = keras.Input(shape=(max_words,), dtype='int32')\n",
    "\n",
    "# Word embedding layer\n",
    "embedded_inputs =keras.layers.Embedding(len(word2id) + 1,\n",
    "                                        embedding_dim,\n",
    "                                        input_length=max_words)(sequence_input)\n",
    "\n",
    "# Apply dropout to prevent overfitting\n",
    "embedded_inputs = keras.layers.Dropout(0.2)(embedded_inputs)\n",
    "\n",
    "# Apply Bidirectional LSTM over embedded inputs\n",
    "lstm_outs = keras.layers.wrappers.Bidirectional(\n",
    "    keras.layers.LSTM(embedding_dim, return_sequences=True)\n",
    ")(embedded_inputs)\n",
    "\n",
    "# Apply dropout to LSTM outputs to prevent overfitting\n",
    "lstm_outs = keras.layers.Dropout(0.2)(lstm_outs)\n",
    "\n",
    "# Attention Mechanism - Generate attention vectors\n",
    "input_dim = int(lstm_outs.shape[2])\n",
    "permuted_inputs = keras.layers.Permute((2, 1))(lstm_outs)\n",
    "attention_vector = keras.layers.TimeDistributed(keras.layers.Dense(1))(lstm_outs)\n",
    "attention_vector = keras.layers.Reshape((max_words,))(attention_vector)\n",
    "attention_vector = keras.layers.Activation('softmax', name='attention_vec')(attention_vector)\n",
    "attention_output = keras.layers.Dot(axes=1)([lstm_outs, attention_vector])\n",
    "\n",
    "# Last layer: fully connected with softmax activation\n",
    "fc = keras.layers.Dense(embedding_dim, activation='relu')(attention_output)\n",
    "output = keras.layers.Dense(len(label2id), activation='softmax')(fc)\n",
    "\n",
    "# Finally building model\n",
    "model = keras.Model(inputs=[sequence_input], outputs=output)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer='adam')\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad67135dcd65940d864521309066ff9fb5b7c9a2"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "d9441f027a63ad3c8b288c6823e073b142c33b34"
   },
   "outputs": [],
   "source": [
    "# Train model 10 iterations\n",
    "model.fit(X, Y, epochs=2, batch_size=64, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b37aca89d92439a9777bb7634dcd12aef2162771"
   },
   "source": [
    "The accuracy on validation data about 93%. Very good result for a classification task with six-classes.\nThe performance can be further improved by training the model a few more iteration.\n\n**Let's look closer to model predictions and attentions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "6a5e94835a8aa88b8609a95e80add37fc1ffd4d7"
   },
   "outputs": [],
   "source": [
    "# Re-create the model to get attention vectors as well as label prediction\n",
    "model_with_attentions = keras.Model(inputs=model.input,\n",
    "                                    outputs=[model.output, \n",
    "                                             model.get_layer('attention_vec').output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "scrolled": true,
    "_uuid": "f7f1a8770b09a221787e38376392ba977172c215"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Select random samples to illustrate\n",
    "sample_text = random.choice(dataset[\"text\"].values.tolist())\n",
    "\n",
    "# Encode samples\n",
    "tokenized_sample = sample_text.split(\" \")\n",
    "encoded_samples = [[word2id[word] for word in tokenized_sample]]\n",
    "\n",
    "# Padding\n",
    "encoded_samples = keras.preprocessing.sequence.pad_sequences(encoded_samples, maxlen=max_words)\n",
    "\n",
    "# Make predictions\n",
    "label_probs, attentions = model_with_attentions.predict(encoded_samples)\n",
    "label_probs = {id2label[_id]: prob for (label, _id), prob in zip(label2id.items(),label_probs[0])}\n",
    "\n",
    "# Get word attentions using attenion vector\n",
    "token_attention_dic = {}\n",
    "max_score = 0.0\n",
    "min_score = 0.0\n",
    "for token, attention_score in zip(tokenized_sample, attentions[0][-len(tokenized_sample):]):\n",
    "    token_attention_dic[token] = math.sqrt(attention_score)\n",
    "\n",
    "\n",
    "# VISUALIZATION\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    return '#%02x%02x%02x' % rgb\n",
    "    \n",
    "def attention2color(attention_score):\n",
    "    r = 255 - int(attention_score * 255)\n",
    "    color = rgb_to_hex((255, r, r))\n",
    "    return str(color)\n",
    "    \n",
    "# Build HTML String to viualize attentions\n",
    "html_text = \"<hr><p style='font-size: large'><b>Text:  </b>\"\n",
    "for token, attention in token_attention_dic.items():\n",
    "    html_text += \"<span style='background-color:{};'>{} <span> \".format(attention2color(attention),\n",
    "                                                                        token)\n",
    "html_text += \"</p>\"\n",
    "# Display text enriched with attention scores \n",
    "display(HTML(html_text))\n",
    "\n",
    "# PLOT EMOTION SCORES\n",
    "emotions = [label for label, _ in label_probs.items()]\n",
    "scores = [score for _, score in label_probs.items()]\n",
    "plt.figure(figsize=(5,2))\n",
    "plt.bar(np.arange(len(emotions)), scores, align='center', alpha=0.5, color=['black', 'red', 'green', 'blue', 'cyan', \"purple\"])\n",
    "plt.xticks(np.arange(len(emotions)), emotions)\n",
    "plt.ylabel('Scores')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd58f4f2b92b103765af428baee13a53d80eb4e9"
   },
   "source": [
    "**We have used an attention mechanism with an LSTM network to recognize emotions in given text.\nWe show that attention mechanism can be useful for classification tasks as well as sequence labeling tasks.\nWe have illustrated the attentions in order to make model predictions interpretable and look fancy.\nEnjoy attentions mechanism in different applications...**\n\n*All feedbacks are welcome.*\n\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
